# rag-document-extraction
Ingest in a pdf insurance document and Q&amp;A on questions regarding what's in the doc with citations.

### Tech Stack:
- ChromaDB
- Docling
- DuckDB

### Models
- Embeddings: bge-large-en-v1.5 (1024-d)
- ReRanker: bge-reranker-base
- Inference: Qwen2.5-32B-Instruct-4bit

### Questions:
- What embedding model to use?
- What chunking strategy?
- How do I pick what LLM to do inference with?

### 1ï¸âƒ£ Put the Entire RFP in Context (No RAG)
You literally feed the whole document (e.g., 30-page PDF) into the modelâ€™s context window.

#### Pros
- âœ… No retrieval errors â€” the model sees everything at once.
- âœ… Works fine for short documents (a few pages).
- âœ… Easier to set up â€” just one prompt.

#### Cons
- âš ï¸ Context-length limits: most models canâ€™t handle 100k+ tokens efficiently.
- âš ï¸ Higher latency and cost (every token counts).
- âš ï¸ Model might get â€œdistractedâ€ â€” it sees too much and misses key details.
- âš ï¸ Answers degrade for long documents â€” LLMs donâ€™t perfectly remember the start of a huge context.

ğŸ‘‰ **Best for:** Quick tests, small PDFs (under 10 pages), or tasks where accuracy isnâ€™t mission-critical.

### 2ï¸âƒ£ Use RAG (Retrieve â†’ Augment â†’ Generate)
You split the RFP into chunks (paragraphs, sections, etc.), store them in a vector database, and retrieve only the most relevant parts at query time.

#### Pros
- âœ… Scales to any document size â€” even hundreds of pages.
- âœ… Faster, cheaper, more memory-efficient.
- âœ… Keeps responses focused â€” model only sees what matters.
- âœ… Easier to trace (â€œHereâ€™s the source paragraph that answered your questionâ€).

#### Cons
- âš ï¸ If your chunking or embeddings arenâ€™t tuned, retrieval might miss the relevant section.
- âš ï¸ Context is limited to whatâ€™s retrieved â€” if the right text isnâ€™t pulled, the model canâ€™t know it.

ğŸ‘‰ **Best for:** Large, structured RFPs or collections of proposals â€” especially if youâ€™ll be asking many different questions (pricing, benefits, eligibility, etc.) across many docs.

## Latest Run Summary
- Targeted 12 in-force attributes using the Docling-driven hierarchical chunking pipeline plus Qwen2.5-32B-Instruct for generation.
- Only 2 of 12 attributes were populated correctly; 10 returned empty or unusable results, indicating the current retrieval windows miss key evidence.
- Chunking approach today: Docling preserves layout â†’ sections aggregated hierarchically â†’ RecursiveCharacter splitter (512/40) â†’ Chroma + bge-large embeddings â†’ reranked with bge-reranker.
- The mix of small, metadata-rich chunks appears to fragment key benefit tables; evidence often lands adjacent to questions, so retrieval fails even with reranking.

## Next Steps on Chunking
- Revisit chunk construction: experiment with product-level slabs (e.g., group all LTD content into a single chunk) so RAG has broader context per attribute query.
- Compare hierarchical spans vs product-level chunks for precision/recall on attribute extraction before adding more heuristics.
- Investigate hybrid retrieval (per-product chunk + smaller supporting snippets) instead of relying solely on fine-grained hierarchical chunks.
- Hold off on â€œfull document in promptâ€ strategy until we benchmark a refined chunking approach; current experience reinforces that better chunking beats 30-page prompts for attribute extraction.
